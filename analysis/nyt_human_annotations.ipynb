{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "repub_df_1 = pd.read_csv('{}/../results/human_annotations/NYTimes Data Collection - Republican_May 27, 2024_15.36.csv'.format(os.getcwd()))\n",
    "repub_df_1 = repub_df_1.drop(index=[0, 1])\n",
    "\n",
    "dem_df_1 = pd.read_csv('{}/../results/human_annotations/NYTimes Data Collection - Democrat_May 27, 2024_15.58.csv'.format(os.getcwd()))\n",
    "dem_df_1 = dem_df_1.drop(index=[0, 1])\n",
    "\n",
    "repub_df_2 = pd.read_csv('{}/../results/human_annotations/NYTimes Data Collection - Republican - V2_May 27, 2024_22.33.csv'.format(os.getcwd()))\n",
    "repub_df_2 = repub_df_2.drop(index=[0, 1])\n",
    "\n",
    "dem_df_2 = pd.read_csv('{}/../results/human_annotations/NYTimes Data Collection - Democrat - V2_May 27, 2024_22.33.csv'.format(os.getcwd()))\n",
    "dem_df_2 = dem_df_2.drop(index=[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_df = pd.concat([dem_df_1, dem_df_2], ignore_index=True, sort=False)\n",
    "repub_df = pd.concat([repub_df_1, repub_df_2], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([dem_df, repub_df],  ignore_index=True, sort=False)\n",
    "col_to_keep = ['QDem_Gend', 'QDem_Race', 'Q_DemRepub', 'QDem_Income', 'Q84', 'QDem_Age'] # demographics\n",
    "combined_df = combined_df[col_to_keep]\n",
    "combined_df.to_csv('{}/../results/human_annotations/nytimes_annotator_demographics.csv'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('{}/../results/human_annotations/NYTIMES_BOOK_DATASET.csv'.format(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualtricsID_to_title, title_to_qualtricsID = {}, {}\n",
    "col_to_keep = ['Q84', 'QDem_Age', 'QDem_Gend', 'QDem_Race', 'Q_DemRepub', 'QDem_Income'] # demographics\n",
    "booktitles = np.array(data['Title and Author'])\n",
    "genres = np.array(data['Genre'])\n",
    "summaries = np.array(data['Summary'])\n",
    "for i, qualtricsID in enumerate([1] + list(np.arange(307, 333))+ list(np.arange(334, 385))+ list(np.arange(386, 408)) + [100] + list(np.arange(408,542))):\n",
    "  col_to_keep.append(str(qualtricsID)+'_Q138')\n",
    "  if str(qualtricsID)+'_Q138' not in repub_df.columns: print(str(qualtricsID)+'_Q138')\n",
    "  qualtricsID_to_title[qualtricsID] = booktitles[i]\n",
    "  title_to_qualtricsID[booktitles[i]] = qualtricsID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "repub_df = repub_df[col_to_keep]\n",
    "dem_df = dem_df[col_to_keep]\n",
    "\n",
    "dem_df['QDem_Age'] = dem_df['QDem_Age'].astype(int)\n",
    "repub_df['QDem_Age'] = repub_df['QDem_Age'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male female\n",
    "male_df = pd.concat([repub_df[repub_df['QDem_Gend']=='Male'], dem_df[dem_df['QDem_Gend']=='Male']], ignore_index=True, sort=False)\n",
    "female_df = pd.concat([repub_df[repub_df['QDem_Gend']=='Female'], dem_df[dem_df['QDem_Gend']=='Female']], ignore_index=True, sort=False)\n",
    "\n",
    "repub_df = repub_df[repub_df['Q_DemRepub']=='Republican']\n",
    "dem_df = dem_df[dem_df['Q_DemRepub']=='Democrat']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131, 206, 165, 172)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(male_df), len(female_df), len(repub_df), len(dem_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_Q138\n",
      "307_Q138\n",
      "308_Q138\n",
      "309_Q138\n",
      "310_Q138\n",
      "311_Q138\n",
      "312_Q138\n",
      "313_Q138\n",
      "314_Q138\n",
      "315_Q138\n",
      "316_Q138\n",
      "317_Q138\n",
      "318_Q138\n",
      "319_Q138\n",
      "320_Q138\n",
      "321_Q138\n",
      "322_Q138\n",
      "323_Q138\n",
      "324_Q138\n",
      "325_Q138\n",
      "326_Q138\n",
      "327_Q138\n",
      "328_Q138\n",
      "329_Q138\n",
      "330_Q138\n",
      "331_Q138\n",
      "332_Q138\n",
      "334_Q138\n",
      "335_Q138\n",
      "336_Q138\n",
      "337_Q138\n",
      "338_Q138\n",
      "339_Q138\n",
      "340_Q138\n",
      "341_Q138\n",
      "342_Q138\n",
      "343_Q138\n",
      "344_Q138\n",
      "345_Q138\n",
      "346_Q138\n",
      "347_Q138\n",
      "348_Q138\n",
      "349_Q138\n",
      "350_Q138\n",
      "351_Q138\n",
      "352_Q138\n",
      "353_Q138\n",
      "354_Q138\n",
      "355_Q138\n",
      "356_Q138\n",
      "357_Q138\n",
      "358_Q138\n",
      "359_Q138\n",
      "360_Q138\n",
      "361_Q138\n",
      "362_Q138\n",
      "363_Q138\n",
      "364_Q138\n",
      "365_Q138\n",
      "366_Q138\n",
      "367_Q138\n",
      "368_Q138\n",
      "369_Q138\n",
      "370_Q138\n",
      "371_Q138\n",
      "372_Q138\n",
      "373_Q138\n",
      "374_Q138\n",
      "375_Q138\n",
      "376_Q138\n",
      "377_Q138\n",
      "378_Q138\n",
      "379_Q138\n",
      "380_Q138\n",
      "381_Q138\n",
      "382_Q138\n",
      "383_Q138\n",
      "384_Q138\n",
      "386_Q138\n",
      "387_Q138\n",
      "388_Q138\n",
      "389_Q138\n",
      "390_Q138\n",
      "391_Q138\n",
      "392_Q138\n",
      "393_Q138\n",
      "394_Q138\n",
      "395_Q138\n",
      "396_Q138\n",
      "397_Q138\n",
      "398_Q138\n",
      "399_Q138\n",
      "400_Q138\n",
      "401_Q138\n",
      "402_Q138\n",
      "403_Q138\n",
      "404_Q138\n",
      "405_Q138\n",
      "406_Q138\n",
      "407_Q138\n",
      "100_Q138\n",
      "408_Q138\n",
      "409_Q138\n",
      "410_Q138\n",
      "411_Q138\n",
      "412_Q138\n",
      "413_Q138\n",
      "414_Q138\n",
      "415_Q138\n",
      "416_Q138\n",
      "417_Q138\n",
      "418_Q138\n",
      "419_Q138\n",
      "420_Q138\n",
      "421_Q138\n",
      "422_Q138\n",
      "423_Q138\n",
      "424_Q138\n",
      "425_Q138\n",
      "426_Q138\n",
      "427_Q138\n",
      "428_Q138\n",
      "429_Q138\n",
      "430_Q138\n",
      "431_Q138\n",
      "432_Q138\n",
      "433_Q138\n",
      "434_Q138\n",
      "435_Q138\n",
      "436_Q138\n",
      "437_Q138\n",
      "438_Q138\n",
      "439_Q138\n",
      "440_Q138\n",
      "441_Q138\n",
      "442_Q138\n",
      "443_Q138\n",
      "444_Q138\n",
      "445_Q138\n",
      "446_Q138\n",
      "447_Q138\n",
      "448_Q138\n",
      "449_Q138\n",
      "450_Q138\n",
      "451_Q138\n",
      "452_Q138\n",
      "453_Q138\n",
      "454_Q138\n",
      "455_Q138\n",
      "456_Q138\n",
      "457_Q138\n",
      "458_Q138\n",
      "459_Q138\n",
      "460_Q138\n",
      "461_Q138\n",
      "462_Q138\n",
      "463_Q138\n",
      "464_Q138\n",
      "465_Q138\n",
      "466_Q138\n",
      "467_Q138\n",
      "468_Q138\n",
      "469_Q138\n",
      "470_Q138\n",
      "471_Q138\n",
      "472_Q138\n",
      "473_Q138\n",
      "474_Q138\n",
      "475_Q138\n",
      "476_Q138\n",
      "477_Q138\n",
      "478_Q138\n",
      "479_Q138\n",
      "480_Q138\n",
      "481_Q138\n",
      "482_Q138\n",
      "483_Q138\n",
      "484_Q138\n",
      "485_Q138\n",
      "486_Q138\n",
      "487_Q138\n",
      "488_Q138\n",
      "489_Q138\n",
      "490_Q138\n",
      "491_Q138\n",
      "492_Q138\n",
      "493_Q138\n",
      "494_Q138\n",
      "495_Q138\n",
      "496_Q138\n",
      "497_Q138\n",
      "498_Q138\n",
      "499_Q138\n",
      "500_Q138\n",
      "501_Q138\n",
      "502_Q138\n",
      "503_Q138\n",
      "504_Q138\n",
      "505_Q138\n",
      "506_Q138\n",
      "507_Q138\n",
      "508_Q138\n",
      "509_Q138\n",
      "510_Q138\n",
      "511_Q138\n",
      "512_Q138\n",
      "513_Q138\n",
      "514_Q138\n",
      "515_Q138\n",
      "516_Q138\n",
      "517_Q138\n",
      "518_Q138\n",
      "519_Q138\n",
      "520_Q138\n",
      "521_Q138\n",
      "522_Q138\n",
      "523_Q138\n",
      "524_Q138\n",
      "525_Q138\n",
      "526_Q138\n",
      "527_Q138\n",
      "528_Q138\n",
      "529_Q138\n",
      "530_Q138\n",
      "531_Q138\n",
      "532_Q138\n",
      "533_Q138\n",
      "534_Q138\n",
      "535_Q138\n",
      "536_Q138\n",
      "537_Q138\n",
      "538_Q138\n",
      "539_Q138\n",
      "540_Q138\n",
      "541_Q138\n"
     ]
    }
   ],
   "source": [
    "dem_values, repub_values, male_values, female_values = [], [], [], []\n",
    "dem_data, repub_data, male_data, female_data = [], [], [], []\n",
    "dem_id_data, repub_id_data, male_id_data, female_id_data = [], [], [], []\n",
    "for i, qualtricsID in enumerate([1] + list(np.arange(307, 333))+ list(np.arange(334, 385))+ list(np.arange(386, 408)) + [100] + list(np.arange(408,542))):\n",
    "    print(str(qualtricsID)+'_Q138')\n",
    "    non_nan_values = np.array(dem_df[str(qualtricsID)+'_Q138'].dropna())\n",
    "    non_nan_prolific_ids = dem_df['Q84'][~pd.isna(dem_df[str(qualtricsID)+'_Q138'])]\n",
    "    dem_data.append(non_nan_values)\n",
    "    dem_id_data.append(dict(zip(non_nan_prolific_ids, non_nan_values)))\n",
    "    dem_values.append(len(non_nan_values))\n",
    "\n",
    "    non_nan_values = np.array(repub_df[str(qualtricsID)+'_Q138'].dropna())\n",
    "    non_nan_prolific_ids = repub_df['Q84'][~pd.isna(repub_df[str(qualtricsID)+'_Q138'])]\n",
    "    repub_data.append(non_nan_values)\n",
    "    repub_id_data.append(dict(zip(non_nan_prolific_ids, non_nan_values)))\n",
    "    repub_values.append(len(non_nan_values))\n",
    "\n",
    "    non_nan_values = np.array(male_df[str(qualtricsID)+'_Q138'].dropna())\n",
    "    non_nan_prolific_ids = male_df['Q84'][~pd.isna(male_df[str(qualtricsID)+'_Q138'])]\n",
    "    male_id_data.append(dict(zip(non_nan_prolific_ids, non_nan_values)))\n",
    "    male_data.append(non_nan_values)\n",
    "    male_values.append(len(non_nan_values))\n",
    "\n",
    "    non_nan_values = np.array(female_df[str(qualtricsID)+'_Q138'].dropna())\n",
    "    non_nan_prolific_ids = female_df['Q84'][~pd.isna(female_df[str(qualtricsID)+'_Q138'])]\n",
    "    female_id_data.append(dict(zip(non_nan_prolific_ids, non_nan_values)))\n",
    "    female_data.append(non_nan_values)\n",
    "    female_values.append(len(non_nan_values))\n",
    "\n",
    "dem_data = np.array(dem_data, dtype=object)\n",
    "repub_data = np.array(repub_data, dtype=object)\n",
    "\n",
    "male_data = np.array(male_data, dtype=object)\n",
    "female_data = np.array(female_data, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all the labels to numbers\n",
    "labels_to_num = {'1: Very unlikely':1 , '2: Somewhat unlikely':2, '3: Somewhat likely':3, '4: Very likely': 4}\n",
    "def get_nums(x):\n",
    "    return labels_to_num[x]\n",
    "\n",
    "vectorized_function = np.vectorize(get_nums)\n",
    "\n",
    "# Apply the vectorized function to the array\n",
    "dem_data_nums = []\n",
    "for i in range(len(dem_data)):\n",
    "    dem_data_nums.append(vectorized_function(dem_data[i]))\n",
    "\n",
    "repub_data_nums = []\n",
    "for i in range(len(repub_data)):\n",
    "    repub_data_nums.append(vectorized_function(repub_data[i]))\n",
    "\n",
    "male_data_nums = []\n",
    "for i in range(len(male_data)):\n",
    "    male_data_nums.append(vectorized_function(male_data[i]))\n",
    "\n",
    "female_data_nums = []\n",
    "for i in range(len(female_data)):\n",
    "    female_data_nums.append(vectorized_function(female_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17570\n"
     ]
    }
   ],
   "source": [
    "num_to_label = {1: '1: Very unlikely', 2: '2: Somewhat unlikely', 3: '3: Somewhat likely', 4: '4: Very likely'}\n",
    "\n",
    "\n",
    "count = 0 \n",
    "all = []\n",
    "proportions_data_annotator_id = defaultdict(dict)\n",
    "for i, book_title in enumerate(booktitles): \n",
    "    book_title=str(book_title)\n",
    "    proportions_data_annotator_id[book_title]['MC_options']=list(['1: Very unlikely', '2: Somewhat unlikely', '3: Somewhat likely', '4: Very likely'])\n",
    "    proportions_data_annotator_id[book_title]['genre']= genres[i]\n",
    "    proportions_data_annotator_id[book_title]['summary']= summaries[i]\n",
    "    \n",
    "    proportions_data_annotator_id[book_title]['Democrat'] = dem_id_data[i]\n",
    "    proportions_data_annotator_id[book_title]['Republican'] = repub_id_data[i]\n",
    "    proportions_data_annotator_id[book_title]['Male'] = male_id_data[i]\n",
    "    proportions_data_annotator_id[book_title]['Female'] = female_id_data[i]\n",
    "\n",
    "    all.append(len(dem_id_data[i].keys()))\n",
    "    all.append(len(repub_id_data[i].keys()))\n",
    "    all.append(len(male_id_data[i].keys()))\n",
    "    all.append(len(female_id_data[i].keys()))\n",
    "    count += len(dem_id_data[i].keys())+len(repub_id_data[i].keys())+len(male_id_data[i].keys())+len(female_id_data[i].keys())\n",
    "\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_keys(keys, lst):\n",
    "    # Initialize a dictionary with all keys set to 0\n",
    "    key_counts = {key: 0 for key in keys}\n",
    "    \n",
    "    # Count occurrences of each key in the list\n",
    "    for item in lst:\n",
    "        if item in key_counts:\n",
    "            key_counts[item] += 1\n",
    "    \n",
    "    return key_counts\n",
    "\n",
    "mc_options = [1, 2, 3, 4]\n",
    "\n",
    "def get_label_to_counts(my_list):\n",
    "    label_to_count = count_keys(mc_options, my_list)\n",
    "    converted_data = {num_to_label[k]: int(v) for k, v in label_to_count.items()}\n",
    "    return converted_data\n",
    "\n",
    "\n",
    "proportions_data = defaultdict(dict)\n",
    "for i, book_title in enumerate(booktitles): \n",
    "    book_title=str(book_title)\n",
    "    proportions_data[book_title]['MC_options']=list(['1: Very unlikely', '2: Somewhat unlikely', '3: Somewhat likely', '4: Very likely'])\n",
    "    proportions_data[book_title]['genre']= genres[i]\n",
    "    proportions_data[book_title]['summary']= summaries[i]\n",
    "    \n",
    "    proportions_data[book_title]['Democrat'] = get_label_to_counts(dem_data_nums[i])\n",
    "    proportions_data[book_title]['Republican'] = get_label_to_counts(repub_data_nums[i])\n",
    "    proportions_data[book_title]['Male'] = get_label_to_counts(male_data_nums[i])\n",
    "    proportions_data[book_title]['Female'] = get_label_to_counts(female_data_nums[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the filename\n",
    "filename = '{}/../NYTIMES/NYTIMES_proportions.json'.format(os.getcwd())\n",
    "\n",
    "# Open the file in write mode and use json.dump to write the dictionary to the file\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(proportions_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save {}_data_nums as data_nums.json \n",
    "data_nums = {\n",
    "    'dem_data_nums': [arr.tolist() for arr in dem_data_nums],\n",
    "    'repub_data_nums': [arr.tolist() for arr in repub_data_nums],\n",
    "    'male_data_nums': [arr.tolist() for arr in male_data_nums],\n",
    "    'female_data_nums': [arr.tolist() for arr in female_data_nums],\n",
    "    'booktitles': booktitles.tolist(), \n",
    "    'summaries': summaries.tolist()\n",
    "}\n",
    "with open('{}/../results/human_annotations/data_nums.json'.format(os.getcwd()), 'w') as file:\n",
    "    json.dump(data_nums, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding top 10 similar books "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from simcse import SimCSE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.84it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.95it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.48it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.23it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.34it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.39it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.42it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.93it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.37it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.96it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "#### TOP 10 similar questions\n",
    "import json\n",
    "data_filename = '{}/nytimes/individual_annotations/NYTIMES_proportions.json'.format(os.getcwd())\n",
    "\n",
    "with open(data_filename, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "question_similarity = {}\n",
    "# map question IDs to embeddings \n",
    "question_ID_to_embedding = {}\n",
    "\n",
    "# go through all the questions in all the waves and calc text embeddings\n",
    "\n",
    "\n",
    "for q_ID in list(data.keys()):\n",
    "    prompt=''\n",
    "    prompt+= \"\\nBook Title: \" + q_ID \n",
    "    prompt+= \"\\nBook Genre: \" + data[q_ID]['genre'] \n",
    "    prompt+= \"\\nBook Summary: \" + data[q_ID]['summary'] \n",
    "    embeddings = np.array(model.encode(prompt).tolist()).reshape(-1, 1)\n",
    "    question_ID_to_embedding[q_ID] = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question_ID in data.keys():\n",
    "    question_IDs, question_IDs_similarity = [], []\n",
    "    \n",
    "    for question_ID_other in data.keys():\n",
    "        if question_ID_other!=question_ID: \n",
    "            question_IDs.append(question_ID_other)\n",
    "\n",
    "            # find similar keys by comparing question ID\n",
    "            a, b = question_ID_to_embedding[question_ID], question_ID_to_embedding[question_ID_other] \n",
    "            norm = np.linalg.norm(a - b) # smaller is more similar \n",
    "            question_IDs_similarity.append(norm)\n",
    "    \n",
    "    sorted_pairs = sorted(zip(question_IDs_similarity, question_IDs))\n",
    "    sorted_listA = [pair[1] for pair in sorted_pairs]\n",
    "    question_similarity[question_ID] = sorted_listA[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data \n",
    "with open('{}/nytimes/question_similarity_top10.json'.format(os.getcwd()), \"w\") as json_file:\n",
    "    json.dump(question_similarity, json_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tatsu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "229e35accff7ff38b3c3cb6fb1801bf5fc89134689dd0a02a51bcb98ebda6289"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
